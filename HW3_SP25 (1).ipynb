{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "5K0BPRZ6sAHL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#**CS6140 - Machine Learning, Spring 2025**\n",
    "##**Homework 3**\n",
    "\n",
    "Submission Instructions:\n",
    "- Please complete this homework assignment in the same notebook provided.\n",
    "- Submit your completed assignment on Canvas by the deadline.\n",
    "\n",
    "Submission Deadline:\n",
    "**Feb 16th, 2025**\n",
    "\n",
    "<p align=\"justify\">\n",
    "Please read the instructions carefully when answering questions and ensure your code works correctly before submission. The grader will run your code for grading the coding questions.\n",
    "</p>\n",
    "\n",
    "This homework has four questions.\n"
   ],
   "metadata": {
    "id": "pUQKKMLHe095"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#@markdown ### Enter your first and last names below:\n",
    "First Name = \"Zhechao\" #@param {type:\"string\"}\n",
    "Last Name = \"Jin\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#**Model Performance Optimization**\n",
    "\n",
    "XYZ company has provided you with a real dataset generated by their machine learning model on their website. The dataset includes the following columns:\n",
    "\n",
    "`item_id` = Unique ID for items  \n",
    "`new_price` = Price for items on XYZ website   \n",
    "`levels` = Different category levels (20000, 30000, 40000, 50000, 60000 and 80000)  \n",
    "`upper_bound` = Model A's prediction for upper bound price  \n",
    "`reliability_score` = The score determines when to switch between Model A and Model B. If the score is less than 0.67, Model B's predictions are used.  \n",
    "`final_ceiling` = Model B's prediction for upper bound price  \n",
    "`resultHigh` = Ground truth\n",
    "\n",
    "\n",
    "The code below provides the current performance of the ML model both overall and grouped by `levels`. Your task is to optimize precision, recall, F1 scores based on the four scenarios outlined in the corresponding questions."
   ],
   "metadata": {
    "id": "7obaU41tsBM1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('labels.csv')\n",
    "m = df.groupby('levels').resultHigh.value_counts()\n",
    "rows = []\n",
    "\n",
    "# Calculate per-level metrics\n",
    "for i in np.sort(df.levels.unique()):\n",
    "    try:\n",
    "        fp = m[i].FP\n",
    "    except KeyError:\n",
    "        fp = 0\n",
    "    try:\n",
    "        tp = m[i].TP\n",
    "    except KeyError:\n",
    "        tp = 0\n",
    "    try:\n",
    "        fn = m[i].FN\n",
    "    except KeyError:\n",
    "        fn = 0\n",
    "    try:\n",
    "        tn = m[i].TN\n",
    "    except KeyError:\n",
    "        tn = 0\n",
    "    totanomalies = tp + fp\n",
    "\n",
    "    try:\n",
    "        precision = tp / (tp + fp)\n",
    "    except ZeroDivisionError:\n",
    "        precision = np.nan\n",
    "    try:\n",
    "        recall = tp / (tp + fn)\n",
    "    except ZeroDivisionError:\n",
    "        recall = np.nan\n",
    "    try:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        f1 = np.nan\n",
    "\n",
    "    rows.append([i, m[i].sum(), precision, recall, f1, tp, fp, fn, tn])\n",
    "\n",
    "# Calculate overall metrics\n",
    "t = df['resultHigh'].value_counts()  # Get overall counts\n",
    "\n",
    "try:\n",
    "    precision = t.TP / (t.TP + t.FP)\n",
    "except ZeroDivisionError:\n",
    "    precision = np.nan\n",
    "try:\n",
    "    recall = t.TP / (t.TP + t.FN)\n",
    "except ZeroDivisionError:\n",
    "    recall = np.nan\n",
    "try:\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "except ZeroDivisionError:\n",
    "    f1 = np.nan\n",
    "\n",
    "# Append overall metrics to the rows\n",
    "rows.append(['Overall', t.sum(), precision, recall, f1, t.TP, t.FP, t.FN, t.get('TN', 0)])\n",
    "\n",
    "# Create the final DataFrame\n",
    "metric_df = pd.DataFrame(rows, columns=['Levels', 'Total Samples', 'Precision', 'Recall', 'F1', 'TP', 'FP', 'FN', 'TN'])\n",
    "\n",
    "# Display the final DataFrame\n",
    "metric_df"
   ],
   "metadata": {
    "id": "aHhj-weCsCfp",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739514541357,
     "user_tz": 480,
     "elapsed": 165,
     "user": {
      "displayName": "Mohammad Toutiaee",
      "userId": "02698600221419502368"
     }
    },
    "outputId": "32247271-1246-44b9-84f3-133784b80449",
    "ExecuteTime": {
     "end_time": "2025-02-17T21:26:22.008610Z",
     "start_time": "2025-02-17T21:26:21.992878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    Levels  Total Samples  Precision    Recall        F1   TP   FP   FN    TN\n",
       "0    20000            370   0.880000  0.409938  0.559322   66    9   95   200\n",
       "1    30000            549   0.852713  0.400000  0.544554  110   19  165   255\n",
       "2    40000           1401   0.729614  0.330097  0.454545  170   63  345   823\n",
       "3    50000            159   0.636364  0.194444  0.297872    7    4   29   119\n",
       "4    60000            425   0.970588  0.375000  0.540984   99    3  165   158\n",
       "5    80000            582   0.637931  0.268116  0.377551   37   21  101   423\n",
       "6  Overall           3486   0.804276  0.352052  0.489735  489  119  900  1978"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Levels</th>\n",
       "      <th>Total Samples</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000</td>\n",
       "      <td>370</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.409938</td>\n",
       "      <td>0.559322</td>\n",
       "      <td>66</td>\n",
       "      <td>9</td>\n",
       "      <td>95</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30000</td>\n",
       "      <td>549</td>\n",
       "      <td>0.852713</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.544554</td>\n",
       "      <td>110</td>\n",
       "      <td>19</td>\n",
       "      <td>165</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40000</td>\n",
       "      <td>1401</td>\n",
       "      <td>0.729614</td>\n",
       "      <td>0.330097</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>170</td>\n",
       "      <td>63</td>\n",
       "      <td>345</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000</td>\n",
       "      <td>159</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.297872</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60000</td>\n",
       "      <td>425</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>165</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>80000</td>\n",
       "      <td>582</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.268116</td>\n",
       "      <td>0.377551</td>\n",
       "      <td>37</td>\n",
       "      <td>21</td>\n",
       "      <td>101</td>\n",
       "      <td>423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overall</td>\n",
       "      <td>3486</td>\n",
       "      <td>0.804276</td>\n",
       "      <td>0.352052</td>\n",
       "      <td>0.489735</td>\n",
       "      <td>489</td>\n",
       "      <td>119</td>\n",
       "      <td>900</td>\n",
       "      <td>1978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Q1: Based on `final_ceiling`**\n",
    "\n",
    "The engineering team suggests that false positives (FP) could be reduced by applying a multiplier to `final_ceiling`. For example, `item_id = 45995610` is currently labeled as an FP because its `new_price` exceeds the `final_ceiling` (i.e., $26.49 > 23.23$). In other words, Model B incorrectly predicts this item as *high price*, leading to the FP label. If we multiply the `final_ceiling` by 1.2, Model B would classify it as a true negative (TN) since $26.49 < (1.2 \\times 23.23)$.\n",
    "\n",
    "Could you reduce FP by adjusting the `final_ceiling` with a multiplier? You could try values in the range [1-3] with increments of 0.1 to optimize, and report the best precision, recall, and F1 scores you achieve per `levels` level and overall (similar to the table above). Also report the best multipliers per level and overall in an additional column in the table.\n",
    "There's no need to focus on reducing false negatives (FN) in this case.\n"
   ],
   "metadata": {
    "id": "Yrt4rReSoQ8W"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T21:26:22.144574Z",
     "start_time": "2025-02-17T21:26:22.089944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"labels.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define the range of multipliers to test\n",
    "multipliers = np.arange(1.0, 3.1, 0.1)\n",
    "\n",
    "# Initialize dictionaries to store best results\n",
    "best_results = {metric: {} for metric in [\"Precision\", \"Recall\", \"F1\"]}\n",
    "best_multipliers = {metric: {} for metric in [\"Precision\", \"Recall\", \"F1\"]}\n",
    "\n",
    "# Iterate through multipliers and calculate metrics\n",
    "for multiplier in multipliers:\n",
    "    # Adjust final ceiling\n",
    "    df[\"adjusted_final_ceiling\"] = df[\"final_ceiling\"] * multiplier\n",
    "\n",
    "    # Update classification based on new ceiling\n",
    "    df[\"new_FP\"] = (df[\"new_price\"] > df[\"adjusted_final_ceiling\"]) & (df[\"resultHigh\"] == \"FP\")\n",
    "    df[\"new_TN\"] = ~df[\"new_FP\"] & (df[\"resultHigh\"] == \"FP\")\n",
    "    df[\"new_TP\"] = df[\"resultHigh\"] == \"TP\"\n",
    "    df[\"new_FN\"] = df[\"resultHigh\"] == \"FN\"\n",
    "\n",
    "    # Aggregate and store best results\n",
    "    for level in list(df[\"levels\"].unique()) + [\"Overall\"]:\n",
    "        level_df = df if level == \"Overall\" else df[df[\"levels\"] == level]\n",
    "\n",
    "        tp, fp, fn = level_df[\"new_TP\"].sum(), level_df[\"new_FP\"].sum(), level_df[\"new_FN\"].sum()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else np.nan\n",
    "\n",
    "        for metric, value in zip([\"Precision\", \"Recall\", \"F1\"], [precision, recall, f1]):\n",
    "            if level not in best_results[metric] or (value > best_results[metric][level]):\n",
    "                best_results[metric][level] = value\n",
    "                best_multipliers[metric][level] = multiplier\n",
    "\n",
    "# Create DataFrames for reporting\n",
    "result_dfs = {\n",
    "    metric: pd.DataFrame.from_dict(best_results[metric], orient=\"index\", columns=[f\"Best {metric}\"])\n",
    "    .assign(**{f\"Best Multiplier ({metric})\": best_multipliers[metric]})\n",
    "    for metric in [\"Precision\", \"Recall\", \"F1\"]\n",
    "}\n",
    "\n",
    "# Display results\n",
    "for metric, df in result_dfs.items():\n",
    "    print(f\"\\nBest {metric} Metrics:\\n\", df)\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Precision Metrics:\n",
      "          Best Precision  Best Multiplier (Precision)\n",
      "40000          0.885417                          2.7\n",
      "20000          0.985075                          2.3\n",
      "30000          0.990991                          2.8\n",
      "80000          1.000000                          2.8\n",
      "60000          1.000000                          2.9\n",
      "50000          1.000000                          1.7\n",
      "Overall        0.953216                          2.9\n",
      "\n",
      "Best Recall Metrics:\n",
      "          Best Recall  Best Multiplier (Recall)\n",
      "40000       0.330097                       1.0\n",
      "20000       0.409938                       1.0\n",
      "30000       0.400000                       1.0\n",
      "80000       0.268116                       1.0\n",
      "60000       0.375000                       1.0\n",
      "50000       0.194444                       1.0\n",
      "Overall     0.352052                       1.0\n",
      "\n",
      "Best F1 Metrics:\n",
      "           Best F1  Best Multiplier (F1)\n",
      "40000    0.480905                   2.7\n",
      "20000    0.578947                   2.3\n",
      "30000    0.569948                   2.8\n",
      "80000    0.422857                   2.8\n",
      "60000    0.545455                   2.9\n",
      "50000    0.325581                   1.7\n",
      "Overall  0.514196                   2.9\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "id": "J28D6YqutS2N"
   },
   "cell_type": "markdown",
   "source": [
    "## **Q2: Based on `upper_bound`**\n",
    "\n",
    "The engineering team suggests that false positives (FP) can be reduced by adjusting the `upper_bound` with a multiplier. For example, `item_id = 304268026` is labeled as an FP because its `new_price` exceeds the `upper_bound` (i.e., $137.31 > 118.47$). In this case, Model A incorrectly classifies the item as *high price*, leading to the FP label. If the `upper_bound` is multiplied by 1.2, Model A would predict it as a true negative (TN), since $137.31 < (1.2 \\times 118.47)$.\n",
    "\n",
    "Could you reduce FP by adjusting the `upper_bound` using a multiplier? Consider testing a range of [1-3], with increments of 0.1, to optimize, and report the best precision, recall, and F1 scores you acheive per `levels` level and overall (similar to the table above). Also report the best multipliers per level and overall in an additional column in the table.\n",
    "There's no need to focus on reducing false negatives (FN) in this case.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T21:26:22.215444Z",
     "start_time": "2025-02-17T21:26:22.160805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "file_path = \"labels.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define the range of multipliers to test for upper_bound\n",
    "multipliers = np.arange(1.0, 3.1, 0.1)\n",
    "\n",
    "# Initialize dictionaries to store best results for upper_bound adjustment\n",
    "best_results_upper = {metric: {} for metric in [\"Precision\", \"Recall\", \"F1\"]}\n",
    "best_multipliers_upper = {metric: {} for metric in [\"Precision\", \"Recall\", \"F1\"]}\n",
    "\n",
    "# Iterate through multipliers and calculate metrics\n",
    "for multiplier in multipliers:\n",
    "    df[\"adjusted_upper_bound\"] = df[\"upper_bound\"] * multiplier\n",
    "\n",
    "    # Update classifications based on new upper_bound\n",
    "    df[\"new_FP\"] = (df[\"new_price\"] > df[\"adjusted_upper_bound\"]) & (df[\"resultHigh\"] == \"FP\")\n",
    "    df[\"new_TN\"] = ~df[\"new_FP\"] & (df[\"resultHigh\"] == \"FP\")\n",
    "    df[\"new_TP\"] = df[\"resultHigh\"] == \"TP\"\n",
    "    df[\"new_FN\"] = df[\"resultHigh\"] == \"FN\"\n",
    "\n",
    "    # Aggregate and store best results\n",
    "    for level in list(df[\"levels\"].unique()) + [\"Overall\"]:\n",
    "        level_df = df if level == \"Overall\" else df[df[\"levels\"] == level]\n",
    "\n",
    "        tp, fp, fn = level_df[\"new_TP\"].sum(), level_df[\"new_FP\"].sum(), level_df[\"new_FN\"].sum()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else np.nan\n",
    "\n",
    "        for metric, value in zip([\"Precision\", \"Recall\", \"F1\"], [precision, recall, f1]):\n",
    "            if level not in best_results_upper[metric] or (value > best_results_upper[metric][level]):\n",
    "                best_results_upper[metric][level] = value\n",
    "                best_multipliers_upper[metric][level] = multiplier\n",
    "\n",
    "# Create DataFrames for reporting\n",
    "result_dfs_upper = {\n",
    "    metric: pd.DataFrame.from_dict(best_results_upper[metric], orient=\"index\", columns=[f\"Best {metric}\"])\n",
    "    .assign(**{f\"Best Multiplier ({metric})\": best_multipliers_upper[metric]})\n",
    "    for metric in [\"Precision\", \"Recall\", \"F1\"]\n",
    "}\n",
    "\n",
    "# Display results\n",
    "for metric, df in result_dfs_upper.items():\n",
    "    print(f\"\\nBest {metric} Metrics (Upper Bound):\\n\", df)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Precision Metrics (Upper Bound):\n",
      "          Best Precision  Best Multiplier (Precision)\n",
      "40000          1.000000                          1.8\n",
      "20000          0.985075                          1.4\n",
      "30000          1.000000                          1.3\n",
      "80000          1.000000                          2.6\n",
      "60000          1.000000                          1.0\n",
      "50000          1.000000                          1.2\n",
      "Overall        0.997959                          2.6\n",
      "\n",
      "Best Recall Metrics (Upper Bound):\n",
      "          Best Recall  Best Multiplier (Recall)\n",
      "40000       0.330097                       1.0\n",
      "20000       0.409938                       1.0\n",
      "30000       0.400000                       1.0\n",
      "80000       0.268116                       1.0\n",
      "60000       0.375000                       1.0\n",
      "50000       0.194444                       1.0\n",
      "Overall     0.352052                       1.0\n",
      "\n",
      "Best F1 Metrics (Upper Bound):\n",
      "           Best F1  Best Multiplier (F1)\n",
      "40000    0.496350                   1.8\n",
      "20000    0.578947                   1.4\n",
      "30000    0.571429                   1.3\n",
      "80000    0.422857                   2.6\n",
      "60000    0.545455                   1.0\n",
      "50000    0.325581                   1.2\n",
      "Overall  0.520490                   2.6\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Q3: Based on `reliability_score`**\n",
    "\n",
    "The engineering team suggests that the `reliability_score` may help reduce false positives (FP) by determining when to switch between `final_ceiling` and `upper_bound`. For example, `item_id = 45995610` is labeled as FP because its `new_price` exceeds the `final_ceiling` (i.e., $26.49 > 23.23$). However, if the `upper_bound` is used for this item, it would be classified as a true negative (TN) since $26.49 < 62.07$. In this case, the `reliability_score` is 0.649277023, so setting a threshold of `reliability_score < 0.65` might allow the use of `upper_bound` instead. However, this is a simplistic approach as it only considers this specific item, and a more generalized threshold should be found to optimize precision, recall, and F1 scores.\n",
    "\n",
    "Can you optimize these metrics by setting a cut-off point on the `reliability_score` to switch between `final_ceiling` and `upper_bound`? Propose cut-off points both at the `levels` level and overall. Also report the best cut-off points per level and overall in an additional column in the table.\n",
    "There's no need to consider the multipliers from Q1 and Q2 in this part.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T21:26:24.657476Z",
     "start_time": "2025-02-17T21:26:22.221448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = \"labels.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert resultHigh column into numerical labels for clarity\n",
    "df[\"resultHigh\"] = df[\"resultHigh\"].map({\"TP\": 1, \"FP\": 0, \"FN\": -1, \"TN\": 2})\n",
    "\n",
    "# compute the best threshold for precision, recall, and F1-score\n",
    "def evaluate_thresholds(metric):\n",
    "    best_results = []\n",
    "\n",
    "    for level in list(df[\"levels\"].unique()) + [\"Overall\"]:\n",
    "        subset = df if level == \"Overall\" else df[df[\"levels\"] == level]\n",
    "        best_threshold, best_value = None, 0\n",
    "\n",
    "        for t in sorted(subset[\"reliability_score\"].unique()):\n",
    "            predictions = np.where(subset[\"reliability_score\"] >= t, subset[\"upper_bound\"], subset[\"final_ceiling\"])\n",
    "\n",
    "            # Update FP to TN based on threshold switch\n",
    "            fp = np.sum((subset[\"new_price\"] > predictions) & (subset[\"resultHigh\"] == 0))\n",
    "            tp = np.sum(subset[\"resultHigh\"] == 1)\n",
    "            fn = np.sum(subset[\"resultHigh\"] == -1)\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "            value = {\"precision\": precision, \"recall\": recall, \"f1\": f1}[metric]\n",
    "\n",
    "            if value > best_value:\n",
    "                best_value, best_threshold = value, t\n",
    "\n",
    "        best_results.append([level, best_threshold, best_value])\n",
    "\n",
    "    return pd.DataFrame(best_results, columns=[\"Levels\", f\"Best Cutoff for {metric.capitalize()}\", f\"Best {metric.capitalize()}\"])\n",
    "\n",
    "# Compute optimal thresholds for Precision, Recall, and F1-score\n",
    "precision_df, recall_df, f1_df = evaluate_thresholds(\"precision\"), evaluate_thresholds(\"recall\"), evaluate_thresholds(\"f1\")\n",
    "\n",
    "# Display the results\n",
    "print(\"Best Cutoffs for Precision:\\n\", precision_df)\n",
    "print(\"\\nBest Cutoffs for Recall:\\n\", recall_df)\n",
    "print(\"\\nBest Cutoffs for F1:\\n\", f1_df)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cutoffs for Precision:\n",
      "     Levels  Best Cutoff for Precision  Best Precision\n",
      "0    40000                   0.262438        0.971429\n",
      "1    20000                   0.262438        0.956522\n",
      "2    30000                   0.295948        0.940171\n",
      "3    80000                   0.295948        0.770833\n",
      "4    60000                   0.262438        1.000000\n",
      "5    50000                   0.295948        0.875000\n",
      "6  Overall                   0.262438        0.947674\n",
      "\n",
      "Best Cutoffs for Recall:\n",
      "     Levels  Best Cutoff for Recall  Best Recall\n",
      "0    40000                0.262438     0.330097\n",
      "1    20000                0.262438     0.409938\n",
      "2    30000                0.295948     0.400000\n",
      "3    80000                0.295948     0.268116\n",
      "4    60000                0.262438     0.375000\n",
      "5    50000                0.295948     0.194444\n",
      "6  Overall                0.262438     0.352052\n",
      "\n",
      "Best Cutoffs for F1:\n",
      "     Levels  Best Cutoff for F1   Best F1\n",
      "0    40000            0.262438  0.492754\n",
      "1    20000            0.262438  0.573913\n",
      "2    30000            0.295948  0.561224\n",
      "3    80000            0.295948  0.397849\n",
      "4    60000            0.262438  0.545455\n",
      "5    50000            0.295948  0.318182\n",
      "6  Overall            0.262438  0.513386\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Q4: Based on `reliability_score`, `upper_bound`, `final_ceiling`**\n",
    "\n",
    "If you've successfully completed the first three questions, you can now combine everything to propose a more comprehensive method. Can you optimize precision, recall, and F1 scores by setting a threshold on `reliability_score` to switch between `final_ceiling` and `upper_bound`, while also determining an optimal multiplier for each? Report the best multipliers along with the cut-off points per level and overall in additional columns in the table.\n",
    "Again, there's no need to focus on reducing false negatives (FN) in this case.\n"
   ],
   "metadata": {
    "id": "SuPpyG8Hzjns"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T21:26:24.689234Z",
     "start_time": "2025-02-17T21:26:24.674444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reload dataset\n",
    "file_path = \"labels.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert resultHigh column into numerical labels for clarity\n",
    "df[\"resultHigh\"] = df[\"resultHigh\"].map({\"TP\": 1, \"FP\": 0, \"FN\": -1, \"TN\": 2})\n",
    "\n",
    "# Best values per level from Q1, Q2, and Q3\n",
    "best_values_per_level = {\n",
    "    \"20000\": {\"threshold\": 0.262438, \"multiplier_fc\": 2.3, \"multiplier_ub\": 1.4},\n",
    "    \"30000\": {\"threshold\": 0.295948, \"multiplier_fc\": 2.8, \"multiplier_ub\": 1.3},\n",
    "    \"40000\": {\"threshold\": 0.262438, \"multiplier_fc\": 2.7, \"multiplier_ub\": 1.8},\n",
    "    \"50000\": {\"threshold\": 0.295948, \"multiplier_fc\": 1.7, \"multiplier_ub\": 1.2},\n",
    "    \"60000\": {\"threshold\": 0.262438, \"multiplier_fc\": 2.9, \"multiplier_ub\": 1.0},\n",
    "    \"80000\": {\"threshold\": 0.295948, \"multiplier_fc\": 2.8, \"multiplier_ub\": 2.6},\n",
    "    \"Overall\": {\"threshold\": 0.262438, \"multiplier_fc\": 2.9, \"multiplier_ub\": 2.6},\n",
    "}\n",
    "\n",
    "# Initialize results storage\n",
    "final_results = []\n",
    "\n",
    "# Iterate over levels and overall\n",
    "for level in list(df[\"levels\"].unique()) + [\"Overall\"]:\n",
    "    subset = df if level == \"Overall\" else df[df[\"levels\"] == level]\n",
    "\n",
    "    # Retrieve best values for this level\n",
    "    best_reliability_threshold = best_values_per_level[str(level)][\"threshold\"]\n",
    "    best_multiplier_fc = best_values_per_level[str(level)][\"multiplier_fc\"]\n",
    "    best_multiplier_ub = best_values_per_level[str(level)][\"multiplier_ub\"]\n",
    "\n",
    "    # Apply best multipliers\n",
    "    adjusted_fc = subset[\"final_ceiling\"] * best_multiplier_fc\n",
    "    adjusted_ub = subset[\"upper_bound\"] * best_multiplier_ub\n",
    "\n",
    "    # Apply the best reliability score threshold\n",
    "    adjusted_predictions = np.where(subset[\"reliability_score\"] >= best_reliability_threshold, adjusted_ub, adjusted_fc)\n",
    "\n",
    "    # Compute metrics\n",
    "    fp = np.sum((subset[\"new_price\"] > adjusted_predictions) & (subset[\"resultHigh\"] == 0))\n",
    "    tp = np.sum(subset[\"resultHigh\"] == 1)\n",
    "    fn = np.sum(subset[\"resultHigh\"] == -1)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    final_results.append([level, best_reliability_threshold, best_multiplier_fc, best_multiplier_ub, precision, recall, f1])\n",
    "\n",
    "# Convert results into DataFrame\n",
    "final_results_df = pd.DataFrame(final_results, columns=[\n",
    "    \"Levels\", \"Best Reliability Threshold\", \"Best Multiplier (Final Ceiling)\",\n",
    "    \"Best Multiplier (Upper Bound)\", \"Precision\", \"Recall\", \"F1 Score\"\n",
    "])\n",
    "\n",
    "# Display results\n",
    "print(\"Final Optimized Results Per Level (Q4):\\n\", final_results_df)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Optimized Results Per Level (Q4):\n",
      "     Levels  Best Reliability Threshold  Best Multiplier (Final Ceiling)  \\\n",
      "0    40000                    0.262438                              2.7   \n",
      "1    20000                    0.262438                              2.3   \n",
      "2    30000                    0.295948                              2.8   \n",
      "3    80000                    0.295948                              2.8   \n",
      "4    60000                    0.262438                              2.9   \n",
      "5    50000                    0.295948                              1.7   \n",
      "6  Overall                    0.262438                              2.9   \n",
      "\n",
      "   Best Multiplier (Upper Bound)  Precision    Recall  F1 Score  \n",
      "0                            1.8   1.000000  0.330097  0.496350  \n",
      "1                            1.4   0.985075  0.409938  0.578947  \n",
      "2                            1.3   1.000000  0.400000  0.571429  \n",
      "3                            2.6   1.000000  0.268116  0.422857  \n",
      "4                            1.0   1.000000  0.375000  0.545455  \n",
      "5                            1.2   1.000000  0.194444  0.325581  \n",
      "6                            2.6   0.997959  0.352052  0.520490  \n"
     ]
    }
   ],
   "execution_count": 24
  }
 ]
}
